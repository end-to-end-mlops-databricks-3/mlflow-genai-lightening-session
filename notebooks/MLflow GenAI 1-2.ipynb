{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5e1680-e79d-4d24-a47b-c6f1a69fb389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://github.com/mlflow/mlflow/pull/14678/files#diff-2af9f81cd25764f6674ae7394b905beb245d4d8fc52dc6e767607581af8aaee2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786884e1-5f34-4c36-80b3-76bad0ea342d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 1: Autologging and Tracing\n",
    "\n",
    "##MLflow for GenAI Guide Overview\n",
    "\n",
    "This is part 1 of a step-by-step guide to using MLflow to experiment with and deploy your generative AI projects.\n",
    "\n",
    "Generative AI projects tend to get complicated quickly. GenAI applications often have many components, including both deterministic functions and GenAI model calls. They often use models from multiple different providers, and may even need to handle different modalities (e.g. text, images, audio, etc.). Testing and iterating on these projects often involves changes to multiple components, including prompts, models, and application logic, and it can be difficult to track and evaluate the impact of these changes.\n",
    "\n",
    "MLflow helps to solve these problems by providing a suite of tools for tracing and visualizing all of your GenAI model calls, evaluating your models and applications, building application logic into custom models, tracking and versioning your models, and deploying your models to production.\n",
    "\n",
    "This is the first part of a four-part guide showing how to use MLflow to experiment with and deploy your generative AI projects. It will cover every phase of the process, from prototyping and informal experimentation through deployment.\n",
    "\n",
    "In this first part of the guide, we will walk through the process of prototyping a GenAI application. We will build a social media style transfer system that generates new posts based on some context and instructions in the style of a set of provided example posts. Part 1 will focus on:\n",
    "\n",
    "- Testing the viability of the project and recording those tests with MLflow tracing\n",
    "- Informal comparison of different models and prompts\n",
    "\n",
    "Subsequent sections will cover:\n",
    "\n",
    "- More rigorous comparison and evaluation of the project with MLflow evaluation metrics\n",
    "- Encapsulating the application logic in a custom model and registering it in the model registry\n",
    "- Deploying the model to staging, evaluating it, and promoting it to production\n",
    "\n",
    "MLflow offers many different tools and approaches that can be used at each step of the process. This guide will introduce some of those options and walk through the thought process behind choosing each particular approach.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1affc89c-a622-4ea3-aeaf-c8b3b8093e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "This guide assumes you have a basic understanding of Python and the MLflow library. You will also need to have the OpenAI Python SDK installed.\n",
    "\n",
    "Inside your Python environment, install the MLflow and OpenAI Python SDK packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee65973-faf9-412d-93c4-57e729708e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/23/17/6f83e6c9d632eb9707663e01f9e74fdd604536fb3ff12ec42da94daf19df/openai-1.73.0-py3-none-any.whl.metadata\n  Downloading openai-1.73.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/be/bd/976b458add04271ebb5a255e992bd008546ea04bb4dcadc042a16279b4b4/jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai) (2.11.3)\nRequirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai) (1.3.1)\nCollecting tqdm>4 (from openai)\n  Obtaining dependency information for tqdm>4 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/57.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from openai) (4.13.2)\nRequirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/18/8d/f052b1e336bb2c1fc7ed1aaed898aa570c0b61a09707b108979d9fc6e308/httpcore-1.0.8-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\nDownloading openai-1.73.0-py3-none-any.whl (644 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/644.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m644.4/644.4 kB\u001B[0m \u001B[31m40.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/73.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.5/73.5 kB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.7/78.7 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/351.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m351.8/351.8 kB\u001B[0m \u001B[31m35.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tqdm, jiter, httpcore, httpx, openai\nSuccessfully installed httpcore-1.0.8 httpx-0.28.1 jiter-0.9.0 openai-1.73.0 tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55fc4e89-43ae-4a99-ac5a-ef892ef53125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start a new MLflow experiment with the following Python code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f973af-f168-4f53-b150-62cd2039ca8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/2056821910552801', creation_time=1744470614881, experiment_id='2056821910552801', last_update_time=1744470614881, lifecycle_stage='active', name='/Shared/genai-social', tags={'mlflow.experiment.sourceName': '/Shared/genai-social',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'basak@marvelousmlops.io',\n",
       " 'mlflow.ownerId': '4485001664756549'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/genai-social\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b11bae2-c03c-4b16-9ae8-35e7300dfa1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Prototyping with MLflow autologging and tracing\n",
    "\n",
    "Before we have written any code, we generally have some idea we want to try out. In this case, we want to answer the question: \"Can a GenAI model generate social media posts that match our brand voice?\" More specifically, we want to see if we can use GenAI to generate posts for the MLflow LinkedIn account in a style that is consistent with the existing posts.\n",
    "\n",
    "The very first step is to try out a few different prompts to see whether this is plausible. At this phase, we are often working in a notebook environment, and are not particularly systematic about structuring our code or recording formal experiments. Still, we might stumble upon some interesting ideas we want to apply later in the project, so having *some* system for recording our tests is helpful. To that end, we can use MLflow's autologging and tracing features to record our experiments.\n",
    "\n",
    "We will use OpenAI's GPT-4o model for experimenting in this stage: it's powerful, reasonably inexpensive, easy to use, and many developers are already familiar with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b12460-efd2-44f4-9f82-2035683ffd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Choosing a Model Interface\n",
    "\n",
    "MLflow offers many different ways to interface with the model—let's look at some of the options and decide which makes the most sense at this point.\n",
    "\n",
    "1. `mlflow.openai.autolog()`: This approach will allow us to use the native OpenAI Python SDK and will automatically log traces of inputs, outputs, errors, and other information to MLflow.\n",
    "2. A manually logged OpenAI model: Logging the model with `mlflow.openai.log_model()` allows us to [log a model with a custom input template](https://mlflow.org/docs/latest/llms/openai/guide/index.html#direct-openai-service-usage) but will require us to manually load the logged model to use it.\n",
    "3. [MLflow AI Gateway](https://mlflow.org/docs/latest/llms/deployments/index.html): We could configure an AI gateway endpoint with multiple models and providers, making it easy to switch between models and providers.\n",
    "\n",
    "At this point in the project, we are not especially concerned with logging specific model configurations or setting up a gateway endpoint to manage multiple models. We just want to get started quickly and record our tests. To that end, we will use the first option: `mlflow.openai.autolog()`. This approach requires just one line of code to enable, and otherwise lets us use the native OpenAI Python SDK for experimentation.\n",
    "\n",
    "You might also wonder why we are using MLflow at all at this early point in the project. We could just use the OpenAI Python SDK—or even ChatGPT—to try out our ideas. One major benefit we have already alluded to is MLflow tracing. Tracing gives us a systematic way to record and learn from our experiments—capturing the exact prompts, parameters, and outputs of every model call. This helps us track what works (and what doesn't), debug issues, and preserve valuable discoveries that we might want to revisit later in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c447fd9a-f1a2-425d-b0ce-b3850220067d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLflow Tracing with Autologging\n",
    "\n",
    "Enable autologging for the OpenAI Python SDK by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8c226f-19be-4fef-8991-7c59f1810d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.openai.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968f60f5-5772-4ecd-b900-272a5de3cbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = dbutils.secrets.get(scope=\"mlflow_genai\", key=\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04fc1686-c6b2-47d4-9b29-b02797f691ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, when we make a call to the OpenAI API, MLflow will automatically log the inputs, outputs, and other information with MLflow tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9322dd4a-2778-4930-a10e-ce95a57229df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-4682c34dfb5242e49e949da3824f17de\"",
      "text/plain": [
       "Trace(request_id=tr-4682c34dfb5242e49e949da3824f17de)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is best Ramen place in Amsterdam?\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee78c3fa-e3ed-44e8-b2ce-b38835b35d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trace = mlflow.get_last_active_trace()\n",
    "mlflow.MlflowClient().set_trace_tag(trace.info.request_id, \"model\", \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960c20d8-8370-4735-b15a-b2f786576396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is a Trace?\n",
    "\n",
    "MLflow tracing provides a record of every call to a model, including the inputs, outputs, errors, and other information. An individual trace is made up of:\n",
    "\n",
    "- **Trace Info**: Metadata about the trace, like timing and status\n",
    "- **Trace Data**: The actual content of the trace, including inputs, outputs, and any intermediate steps\n",
    "\n",
    "You can learn much more about MLflow tracing [here](https://mlflow.org/docs/latest/llms/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba2ba3d-0bbb-4177-98d6-faf756124f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generating our First Post\n",
    "\n",
    "Now that we have a basic system for logging our experiments, let's see if we can get a basic social media style transfer system working. As a quick reminder, we want to build a GenAI system that generates new LinkedIn posts based on some provided context and instructions in a style that is consistent with a set of provided example posts.\n",
    "\n",
    "We will need:\n",
    "\n",
    "- An example post to use as a style reference (eventually, we will use a set of multiple examples)\n",
    "- A source document or website with content to use as a source of information for a new post\n",
    "- A template prompt to tie the above information together\n",
    "- A system prompt explaining the task to the model\n",
    "\n",
    "Let's try the following. Feel free to experiment with your own prompts and ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28207857-b9ea-470b-8757-fc94b33b23b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Prompts:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796366bc-954b-4450-a18c-c230969c2880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_1 = \"\"\"You are a social media content specialist who can precisely match writing styles. Your task is to:\n",
    "\n",
    "1. Analyze the provided example post(s) to understand their style and tone\n",
    "2. Generate a new LinkedIn post about the given topic that perfectly matches this style\n",
    "3. Return only the generated post, nothing else.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_template = \"\"\"\n",
    "\n",
    "example posts:\n",
    "\n",
    "{example_posts}\n",
    "\n",
    "topic:\n",
    "\n",
    "{topic}\n",
    "\n",
    "additional instructions:\n",
    "\n",
    "{additional_instructions}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676ae45d-b78f-4628-951d-cbcbf38c8590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example Post:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85af57b7-1c82-4994-9f85-2c16bb1b1980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "example_post = \"\"\"MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.\n",
    "\n",
    "Now you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions—no need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.\n",
    "\n",
    "This means:\n",
    "\n",
    "🔍 Easier debugging during prototyping\n",
    "\n",
    "🔌 More flexible integration options\n",
    "\n",
    "🎯 Works with or without other MLflow features\n",
    "\n",
    "Learn more:\n",
    "\n",
    "📚 Docs: https://lnkd.in/gyBzcrDr\n",
    "\n",
    "📝 Release notes: https://lnkd.in/gBrNQfFC\n",
    "\n",
    "#MachineLearning #AI #LLMs #LLMOps #Evals\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da9126c-cad2-401b-85ec-03fe65ffd1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**New Post Topic:**\n",
    "\n",
    "We will mostly get source information from the MLflow docs and blog posts. Let's write a quick helper function to get the text from a webpage and convert it to markdown. We will use the [`markdownify`](https://github.com/matthewwithanm/python-markdownify) library to do this—you can install it with `pip install markdownify`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e393b5e4-92b9-43d0-bf6b-620ef7f7693a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdownify in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a7aa4b3f-1bc1-4c49-862b-f80d676a8203/lib/python3.11/site-packages (1.1.0)\nRequirement already satisfied: beautifulsoup4<5,>=4.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a7aa4b3f-1bc1-4c49-862b-f80d676a8203/lib/python3.11/site-packages (from markdownify) (4.13.3)\nRequirement already satisfied: six<2,>=1.15 in /usr/lib/python3/dist-packages (from markdownify) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a7aa4b3f-1bc1-4c49-862b-f80d676a8203/lib/python3.11/site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from beautifulsoup4<5,>=4.9->markdownify) (4.13.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5eccd7-914f-40b1-9122-00a8f3bb00f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from markdownify import markdownify\n",
    "\n",
    "def webpage_to_markdown(url):\n",
    "    # Get webpage content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Convert to markdown\n",
    "    markdown_content = markdownify(html_content)\n",
    "\n",
    "    return markdown_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1335047d-ba43-4248-81f6-c74b07f9c6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can use this function to get the text from the MLflow docs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf02d3e-0c65-4306-8fa6-dbbadbde6551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://mlflow.org/docs/latest/llms/chat-model-intro/index.html\"\n",
    "markdown_content = webpage_to_markdown(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5398ab5d-0a38-4e8f-b341-a6ee46476229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We now have a system prompt, a template for our user prompt, an example post, and a source of information. Now we just need some custom instructions and a helper function to assemble the final set of messages to send to the model.\n",
    "\n",
    "**Additional Instructions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbab915c-5d25-4a8e-98b6-42b9314f02b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_instructions = \"\"\"This post will be written for the MLflow LinkedIn account.\n",
    "\n",
    "Maintain a professional but approachable tone. Developers are the primary audience.\n",
    "\n",
    "No marketing slop.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13fd1f9c-0519-453b-9926-e3663ad5573d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Prompt Formatting Helper Function:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78477b3f-e1e4-43ab-a64e-60f217d982a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    system, user_template, example_posts, topic, additional_instructions\n",
    "):\n",
    "    \"\"\"Generate a prompt for the LLM based on the example posts, topic, and additional instructions.\"\"\"\n",
    "    example_posts = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\n{post}\" for i, post in enumerate(example_posts)]\n",
    "    )\n",
    "    prompt = user_template.format(\n",
    "        example_posts=example_posts,\n",
    "        topic=topic,\n",
    "        additional_instructions=additional_instructions,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88dccf73-320f-4273-9570-053fd456461d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-da1e040687d849d2a043c25ecbe84d9c\"",
      "text/plain": [
       "Trace(request_id=tr-da1e040687d849d2a043c25ecbe84d9c)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = generate_prompt(system_prompt_1, user_template, [example_post], [markdown_content], additional_instructions)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa337718-4f93-4a63-8c82-574b3eb87bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First Result\n",
    "\n",
    "This is a reasonable first result, and gives us some confidence that this project is feasible. If you're following along, we encourage you to try out your own prompts and ideas. What is working? What isn't? What hypotheses can you come up with that you would like to test more rigorously?\n",
    "\n",
    "### Tagging Traces\n",
    "\n",
    "Now, as we iterate on the prompts and helper functions, the results will automatically be logged to MLflow tracing. We can add tags to the traces to help us organize our experiments. For example, suppose we want to add tags to record the platform for which we are generating posts. We can add a tag to the last active trace as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb586469-5ca0-4a74-95fa-4e6641008d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6620300559654609>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m trace \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mget_last_active_trace()\n",
       "\u001B[1;32m      2\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mMlflowClient()\u001B[38;5;241m.\u001B[39mset_trace_tag(trace\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mrequest_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplatform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLinkedIn\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mlflow' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'mlflow' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mlflow' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-6620300559654609>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trace \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mget_last_active_trace()\n\u001B[1;32m      2\u001B[0m mlflow\u001B[38;5;241m.\u001B[39mMlflowClient()\u001B[38;5;241m.\u001B[39mset_trace_tag(trace\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mrequest_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplatform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLinkedIn\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'mlflow' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace = mlflow.get_last_active_trace()\n",
    "mlflow.MlflowClient().set_trace_tag(trace.info.request_id, \"platform\", \"LinkedIn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b85bde5-6ccb-4bd6-bb11-7c6393173ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that we can also add tags to the trace in the UI. We can use the tags to [search and filter](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.search_traces) traces.\n",
    "\n",
    "### Tracing additional models\n",
    "\n",
    "We might also be interested in trying out some different models. For example, we might want to try the Gemini 2.0 Flash model as an alternative to GPT-4o. We want to make sure that calls to new models are traced alongside our OpenAI traces. We have a couple of options for making that happen:\n",
    "\n",
    "1. Use the [`google-generativeai`](https://ai.google.dev/api?lang=python) library and the `mlflow.gemini.autolog()` function to trace the Gemini model calls.\n",
    "2. Use Gemini [via the OpenAI SDK](https://ai.google.dev/gemini-api/docs/openai), so calls will be traced because of our existing autologging setup.\n",
    "\n",
    "We will use the second option here—it ensures that our existing helper functions and prompt formatting will work and that our traces will be captured in the same format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dcf49e9-0516-4a58-a1c3-d2af77d54386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Set up Gemini:**\n",
    "\n",
    "We can set up an OpenAI client for Gemini as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f751a25b-3ba9-43c3-9990-eba489675758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = dbutils.secrets.get(scope=\"mlflow_genai\", key=\"GEMINI_API_KEY\")\n",
    "\n",
    "gemini_client = OpenAI(\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f3bd72-d3ea-4d0e-8616-ec3cc87584f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: [Gemini is now accessible from the OpenAI Library](https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6a0ab2-6247-4aaf-9b4a-c8513e1f3192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-901881af8a4345f4a1b8c5be0148d143\"",
      "text/plain": [
       "Trace(request_id=tr-901881af8a4345f4a1b8c5be0148d143)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = gemini_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e14abe0a-fd78-48b0-b0f4-829165a0da79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While we're at it, let's tag the latest trace with both the platform and the model provider. The model name is already captured in the trace, but tagging will make it easier to distinguish in the UI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc48d3de-8779-4177-bf9a-0a46d2c16bf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trace = mlflow.get_last_active_trace()\n",
    "mlflow.MlflowClient().set_trace_tag(trace.info.request_id, \"platform\", \"LinkedIn\")\n",
    "mlflow.MlflowClient().set_trace_tag(trace.info.request_id, \"model\", \"gemini-2.0-flash-exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7b8490-4568-43b5-92e9-773840b541bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gemini Trace\n",
    "\n",
    "From here, we can continue to try out different models and prompts, tagging traces to track our experiments across platforms, models, and prompts.\n",
    "\n",
    "While these informal experiments are great for developing intuitions and initial hypotheses, they don't provide a systematic way to evaluate our application. Throughout development, we'll want to make various changes to our system—but without rigorous evaluation, measuring their impact is mostly guesswork. Building out a proper evaluation suite will help us validate our current hypotheses and give us a framework for measuring future changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e296dfcc-a47a-4039-8067-83ea5671112b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this first part of a four-part guide detailing how MLflow integrates with a GenAI project, from conception through deployment, we prototyped and evaluated our AI system. In particular, we saw how:\n",
    "\n",
    "- MLflow autologging let us capture traces of our early informal experiments with just one line of code: `mlflow.openai.autolog()`.\n",
    "- We can use MLflow tracing to capture and learn from informal experiments\n",
    "- We can compare models from different providers and tag their respective traces\n",
    "\n",
    "The next section of this guide shows how to build on our informal experiments and develop a more rigorous evaluation suite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e588cada-d3a0-4ccc-91a1-8b20db3a9bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 2: Structured Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91022c1a-c732-4142-9f91-892488c8cb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "MLflow for GenAI Guide Overview\n",
    "\n",
    "Now that we have some promising initial results, it's time to move from informal experimentation to systematic evaluation. Our goal is to build an evaluation framework that will help us:\n",
    "\n",
    "- Compare different models and prompts quantitatively\n",
    "- Measure key qualities like factual accuracy and style consistency\n",
    "- Create benchmarks we can use to measure future improvements\n",
    "\n",
    "We will use MLflow's evaluation tools to build this framework and run our first formal comparison of candidate systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fc0e1d-4b49-4f1f-a390-7af9d81d04a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What are we trying to evaluate?\n",
    "\n",
    "The first step in evaluating our system is to formalize our questions and hypotheses. This is important: don't start coding an evaluation suite until you know what you want to test. Here are some questions you might consider before starting:\n",
    "\n",
    "- Which variable(s) are we holding constant? Which are we changing? For example: we might choose to use the same set of example posts while varying the prompt or model.\n",
    "- What does it mean for one generated post to be \"better\" than another? If you were to design a rubric for comparing the quality of the generated posts, what would it look like?\n",
    "- What are the most important issues to avoid when generating posts? For example, we probably want to make sure to avoid generating posts that contain factual errors or harmful/toxic content.\n",
    "\n",
    "Let's suppose our testing has identified two candidate prompts. Furthermore, we want to compare the performance of the Gemini 2.0 Flash model to the GPT-4o model. How do we want to judge which prompt and model is better? While there could be many different ways to evaluate our system, let's start with a few basics:\n",
    "\n",
    "1. We want to make sure the generated posts are grounded in the source information. All factual information in the generated post should be present in the source information.\n",
    "2. We want to make sure the generated posts do not contain any harmful or toxic content. Though it is unlikely that a model will generate harmful content on purpose, it's still worth checking for.\n",
    "3. We want to make sure the generated posts are in the style of the example posts.\n",
    "\n",
    "This should be enough to get started. Over time, as we get more data and feedback, we can refine our evaluation suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355ba294-baa5-4fc8-b174-ea700f9d9fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mapping our Evaluation Criteria to Metrics\n",
    "\n",
    "Mapping evaluation criteria to metrics\n",
    "\n",
    "Now that we know what criteria we want to use to compare our generated posts, we can use MLflow to build our evaluation suite using [MLflow's LLM Evaluation features](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html).\n",
    "\n",
    "The first step is to map our evaluation criteria to MLflow metrics. An [MLflow metric](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#llm-evaluation-metrics) is either a heuristic-based function that calculates a numerical score (like ROUGE or BLEU) or an LLM-as-a-Judge metric that uses another LLM to evaluate and score model outputs. We will largely rely on LLM-as-a-Judge metrics because they can better evaluate qualities like writing style, factual consistency, and tone that are crucial for social media content.\n",
    "\n",
    "To check for groundedness and toxicity, we can use the built-in [faithfulness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness) and [toxicity](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html?highlight=toxicity#mlflow.metrics.toxicity) metrics. There is no built-in metric for style similarity, so we can create our own.\n",
    "\n",
    "Ultimately, we will want to set up our evaluation suite to run all at once with `mlflow.evaluate()`, but we can start by testing our metrics one at a time. MLflow metrics work as Python callable functions, making it easy to test them individually as we are configuring our evaluation suite. Let's work through them one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b3855f-7e64-45ae-927f-718859105991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Faithfulness\n",
    "\n",
    "*Faithfulness* is a metric that checks whether the generated post is grounded in the source information. Note that we could also consider using the [answer_correctness](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_correctness) metric, which checks whether the generated post is correct. There is a subtle difference between the two: *faithfulness* checks whether the generated post is grounded in the source information, while *answer_correctness* checks whether the generated post is correct relative to a target answer. We are more interested in the former, so we will use the `faithfulness` metric.\n",
    "\n",
    "Here's how it works. We will stick with the default judge model, GPT-4o, though you can [choose your preferred model to use as a judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#selecting-the-judge-model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b34cb9d-e06f-46fa-93f9-c34fcf326185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aiohttp) (1.19.0)\nRequirement already satisfied: idna>=2.0 in /databricks/python3/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.4)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb45082-3e9f-4a14-8d09-9c57d86174f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb697474-4122-4a55-b84a-8ac6816b9e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207737601a6f4defa49c349ce4412255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricValue(scores=[5], justifications=[\"All claims in the output are directly supported by the provided context. The output accurately describes the features and benefits of MLflow's `ChatModel` class, its comparison with `PythonModel`, and the tutorial's content, all of which are consistent with the context provided. Therefore, the faithfulness score is 5.\"], aggregate_results={'mean': 5.0, 'variance': 0.0, 'p90': 5.0})\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics.genai import faithfulness\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = dbutils.secrets.get(scope=\"mlflow_genai\", key=\"OPENAI_API_KEY\")\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=\"openai:/gpt-4o\"\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(faithfulness_metric(predictions=result,\n",
    "                    inputs=additional_instructions, # ignored\n",
    "                    context=markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "805b91f9-de75-4852-a50f-9512f3d65eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Toxicity\n",
    "\n",
    "We can use `toxicity` similarly. Note that we do not pass a model to the `toxicity` metric: it uses the specialized `facebook/roberta-hate-speech-dynabench-r4-target` model for toxicity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c3d600-81ee-413d-bee3-6f55616394bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:00:29 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics import toxicity\n",
    "\n",
    "toxicity_metric = toxicity()\n",
    "\n",
    "toxicity_score = toxicity_metric(predictions=result)\n",
    "# toxicity_score.aggregate_results[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf92b0b-d30b-46d5-a125-fd5e8fbf5878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "toxicity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "981d647f-e0d9-403b-ab49-f7f09a0087b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Style Similarity\n",
    "\n",
    "For style similarity, we need to create a custom metric. We will use the `make_genai_metric` function to create a custom metric that compares the generated post to a set of example posts and returns a score between 0 and 5.\n",
    "\n",
    "The first step in creating an effective metric is to define some examples with inputs, scores, and justifications. We will use the `EvaluationExample` class to define our examples.\n",
    "\n",
    "First, let's copy over some more example posts from the [MLflow LinkedIn account](https://www.linkedin.com/company/mlflow-org/posts/). We will use these posts throughout the rest of this guide (recall: we will be keeping the examples fixed but varying the prompt and model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a418a70-fbb1-4868-94ee-4adc808de373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "post_example_1 = \"\"\"MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.\n",
    "Now you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions—no need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.\n",
    "This means:\n",
    "🔍 Easier debugging during prototyping\n",
    "🔌 More flexible integration options\n",
    "🎯 Works with or without other MLflow features\n",
    "Check it out in action ⬇️\n",
    "Learn more:\n",
    "📚 Docs: https://lnkd.in/gyBzcrDr\n",
    "📝 Release notes: https://lnkd.in/gBrNQfFC\n",
    "#MachineLearning #AI #LLMs #LLMOps #Evals\"\"\"\n",
    "\n",
    "post_example_2 = \"\"\"If you're already building with Python ML libraries, adding mlflow.autolog() to your code instantly gives you production-grade experiment tracking, model management, and observability—no extra infrastructure or code changes needed.\n",
    "The automatic logging works across a remarkable breadth of libraries—from GenAI frameworks like LangChain, OpenAI, and LlamaIndex to traditional ML and deep learning libraries like PyTorch, scikit-learn, and Fastai.\n",
    "MLflow's autolog feature changes this equation. With a single line—mlflow.autolog()—you get automatic logging of:\n",
    "📊 Training metrics and parameters for scikit-learn, PyTorch, many other ML frameworks\n",
    "🔄 LLM traces, prompts, responses, and tool calls for OpenAI and LangChain\n",
    "📦 Model signatures and artifacts\n",
    "💾 Dataset information and example inputs\n",
    "The best part is that it works out of the box with the most popular libraries in the Python ML ecosystem: no need to modify your existing training code or add manual logging statements.\n",
    "Read more: https://lnkd.in/e_aTp6HH\n",
    "#machinelearning #mlops #ai #llmops\"\"\"\n",
    "\n",
    "post_example_3 = \"\"\"New tutorial: Step-by-step guide to building a tool-calling LLM application using MLflow's ChatModel wrapper and tracing system.\n",
    "This tutorial shows you how to:\n",
    "🔧 Create a tool-calling model using mlflow.pyfunc.ChatModel\n",
    "🔄 Implement OpenAI function calling with automatic input/output handling\n",
    "🔍 Add comprehensive tracing to debug multi-step LLM interactions\n",
    "🚀 Deploy your model with full MLflow lifecycle management\n",
    "The guide includes a practical example building a weather information agent, showing how ChatModel simplifies complex LLM patterns while providing enterprise-grade observability.\n",
    "Check out the complete tutorial here: https://lnkd.in/gdTw8N2S\n",
    "#MLOps #AIEngineering #LLMOps #AI\"\"\"\n",
    "\n",
    "example_posts = [post_example_1, post_example_2, post_example_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60927547-f537-4f61-9f4c-720d5307b5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similar_post = \"\"\"\n",
    "MLflow's ChatModel and PythonModel classes serve different needs when deploying GenAI applications. Here's when to use each:\n",
    "ChatModel simplifies GenAI deployment with standardized OpenAI-compatible interfaces. This means:\n",
    "🔗 Immediate compatibility with existing OpenAI-based tools and workflows\n",
    "🚀 Pre-defined model signatures that work out of the box\n",
    "📊 Streamlined integration with MLflow's tracking and evaluation features\n",
    "PythonModel is your choice when you need complete control over:\n",
    "🛠️ Custom input/output schemas for specialized use cases\n",
    "🔄 Complex data transformations beyond standard chat patterns\n",
    "⚙️ Fine-grained model behavior and deployment configurations\n",
    "For most conversational AI applications, ChatModel's standardized approach helps you avoid common deployment pitfalls while maintaining consistent interfaces across your GenAI services. Consider PythonModel when your use case requires specialized data handling or custom interaction patterns.\n",
    "See the comment below for links to in-depth tutorials on ChatModel 👇 \n",
    "#MLflow #LLMOps #MachineLearning #GenAI\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38102294-e2f9-40aa-b51c-b267f8892aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dissimilar_post = \"\"\"\n",
    "🔥 HOLY MOLY! MLflow Just Dropped Something INSANE for AI Deployment! 🤯\n",
    "TWO EPIC WAYS to deploy your next-gen AI:\n",
    "1️⃣ ChatModel: The No-BS Fast Track!\n",
    "INSTANT OpenAI compatibility 🤝\n",
    "Zero headaches, works RIGHT NOW 🚀\n",
    "All the tracking & metrics you're craving 📈\n",
    "2️⃣ PythonModel: For When You Need to GO WILD!\n",
    "Customize EVERYTHING 🎨\n",
    "Transform data like a BOSS 💪\n",
    "Ultimate control = Ultimate POWER! ⚡️\n",
    "Don't sleep on this update! Your AI deployment game is about to get ABSOLUTELY CRACKED! 🚀✨\n",
    "#MLflowGang #AIrevolution #FutureIsNow #TechTwitter\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea8450d-70d6-4ae0-b424-1e389a3c3861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can set up two `EvaluationExample` objects, one for the similar post and one for the dissimilar post. These need to include the input, output, examples, score, and justification.\n",
    "\n",
    "Note that we provide examples via the `grading_context` argument. Furthermore, we're just passing the \"additional instructions\" as the input. These are not relevant to the style similarity metric. We want to be careful not to send the full message history or the source information as this could impair the metric's ability to evaluate the style similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7b51bc-e5ad-441d-b011-e9a894b25276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import EvaluationExample\n",
    "\n",
    "evaluation_example_1 = EvaluationExample(\n",
    "    input=additional_instructions,\n",
    "    output=similar_post,\n",
    "    grading_context={\"examples\": example_posts},\n",
    "    score=5,\n",
    "    justification=\"This post is a perfect match to the style of the example posts.\"\n",
    ")\n",
    "\n",
    "evaluation_example_2 = EvaluationExample(\n",
    "    input=additional_instructions,\n",
    "    output=dissimilar_post,\n",
    "    grading_context={\"examples\": example_posts},\n",
    "    score=1,\n",
    "    justification=(\"The post earns a 1/5 for maintaining the basic bullet-point structure and use of emojis, \"\n",
    "                   \"but significantly overplays the informal tone with phrases like 'HOLY MOLY!' and 'fam' \"\n",
    "                   \"that aren't present in the examples. While the example posts balance professional \"\n",
    "                   \"enthusiasm with technical detail, this submission sacrifices information density for \"\n",
    "                   \"excessive hype and casual language that goes well beyond the controlled informality shown \"\n",
    "                   \"in the reference posts.\"\n",
    "                   )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6072fac3-1bca-4535-9413-c9f73899cf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we have our examples, we can use the [`make_genai_metric`](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.make_genai_metric) function to create a custom metric. There are a few key components we need to provide in order to define a new GenAI metric:\n",
    "\n",
    "- a definition, which describes the basic intent of the metric\n",
    "- a grading prompt, which describes the scoring system and provides any other necessary notes\n",
    "- the evaluation examples, which we created above\n",
    "\n",
    "In this case, we also set a custom model: we're using the Anthropic Claude 3.5 Sonnet model for this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d33b5ee-a43e-473d-9d51-4c587060ee59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import make_genai_metric\n",
    "\n",
    "style_similarity_metric = make_genai_metric(\n",
    "    name=\"style_similarity\",\n",
    "    definition=(\n",
    "        \"Style similarity measures how well a generated social media post matches the style, tone, \"\n",
    "        \"and vocabulary of provided example posts. This includes analyzing the similarity of tone, \"\n",
    "        \"word choice, punctuation, sentence structure, and stylistic elements like hashtags and emojis. \"\n",
    "        \"Content similarity should not factor into the style similarity score. Post length is of minimal importance.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Style Similarity: Score the generated post's similarity to the example posts on a scale from 0 to 5:\\n\"\n",
    "        \"- Score 0: No stylistic similarity at all\\n\"\n",
    "        \"- Score 1: Minimal stylistic similarity\\n\"\n",
    "        \"- Score 2: Some stylistic elements match but significant differences exist\\n\"\n",
    "        \"- Score 3: Moderate stylistic similarity in tone, vocabulary, or structure\\n\"\n",
    "        \"- Score 4: High stylistic similarity across most elements\\n\"\n",
    "        \"- Score 5: Could be written by the same author\\n\\n\"\n",
    "        \"Consider:\\n\"\n",
    "        \"- Tone: similarity in voice and attitude\\n\"\n",
    "        \"- Vocabulary: similarity in word choice and complexity\\n\"\n",
    "        \"- Style: similarity in punctuation, sentence structure, hashtags, and emojis\"\n",
    "    ),\n",
    "    examples=[evaluation_example_1, evaluation_example_2],\n",
    "    version=\"v1\",\n",
    "    model=\"anthropic:/claude-3-5-sonnet-20241022\",\n",
    "    parameters={\"temperature\": 0.0, \"max_tokens\": 1000},\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    grading_context_columns=[\"examples\"],\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4cd416-83b1-4c18-8093-57680b34d82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = dbutils.secrets.get(scope=\"mlflow_genai\", key=\"ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a0eb24-8ac6-4bc1-9ee6-0e03c1d627d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54e61258af243879a3a7d38d97a23f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MetricValue(scores=[2], justifications=['The output maintains some basic structural elements from the example posts (bullet points, technical content), but lacks many key stylistic elements present in the examples. It\\'s missing emojis, hashtags, and the more engaging, direct tone of the examples. The writing is overly formal and documentation-like, using phrases like \"conversely\" and \"mitigates common deployment challenges\" where the examples use more approachable language. The post also lacks the clear call-to-action links and interactive elements (\"Check it out ⬇️\") that characterize the example posts.'], aggregate_results={'mean': 2.0, 'variance': 0.0, 'p90': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "too_formal_example = \"\"\"MLflow has introduced distinct deployment paradigms through its ChatModel and PythonModel classes, each serving specific implementation requirements in GenAI applications.\n",
    "ChatModel implements a standardized deployment framework utilizing OpenAI-compatible interfaces, offering several advantages:\n",
    "Full compatibility with existing OpenAI infrastructure and workflows\n",
    "Implementation of predefined model signatures ensuring immediate functionality\n",
    "Seamless integration with MLflow's comprehensive tracking and evaluation systems\n",
    "Conversely, PythonModel provides advanced customization capabilities for specialized requirements:\n",
    "Implementation of bespoke input/output schemas\n",
    "Advanced data transformation protocols beyond standard conversational patterns\n",
    "Granular control over model behavior and deployment specifications\n",
    "For standard conversational AI implementations, ChatModel's structured approach mitigates common deployment challenges while maintaining consistent interfaces across GenAI services. PythonModel remains the optimal choice for implementations requiring specialized data handling protocols or custom interaction patterns.\n",
    "For detailed implementation guidelines, please refer to the accompanying documentation.\n",
    "Reference: MLflow Documentation\n",
    "\"\"\"\n",
    "\n",
    "style_similarity_metric(predictions=too_formal_example,\n",
    "                    inputs=additional_instructions,\n",
    "                    examples=[example_posts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92164d83-7396-414f-99d3-bb6a2f28704d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tying it all together with `mlflow.evaluate()`\n",
    "\n",
    "Now that we have our metrics, it's time to structure our evaluation suite and run our experiments. As we mentioned earlier, we want to identify the best model and system prompt combination.\n",
    "\n",
    "For the purposes of this guide, we will use our `webpage_to_markdown` function to generate a set of ten different sources for posts. We will then generate four different posts for each source, using two different prompts and two different models. We will save the results of our experiments as a Pandas DataFrame and use the `mlflow.evaluate()` function to evaluate the generated posts using our three metrics. In a real-world scenario, we would want to run a larger experiment with more examples and, potentially, a wider range of candidate models and prompts.\n",
    "\n",
    "Note that we could, instead, simply track these experiments as individual MLflow runs. However, using `mlflow.evaluate()` provides a more structured way to compare models and prompts across multiple metrics, with built-in support for viewing and analyzing evaluation results in the MLflow UI.\n",
    "\n",
    "We also have a few different options for how to use `mlflow.evaluate()`. We can pass the model to `mlflow.evaluate()`, which will call on the model to generate fresh predictions each time we run the evaluation suite, or we can pass a dataset including all the necessary inputs, outputs, and context. We will go with the latter approach. This way, if we need to debug or update our evaluation setup, we do not need to re-run the generation step: we can simply run the updated evaluations on the static dataset.\n",
    "\n",
    "First, we'll set up our key experiment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff134cbb-d0df-42c5-9eb7-8178a595499f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system_prompt_2 = \"\"\"You are a social media content specialist with expertise in matching writing styles and voice across platforms. Your task is to:\n",
    "1. Analyze the provided example post(s) by examining:\n",
    "   - Writing style, tone, and voice\n",
    "   - Sentence structure and length\n",
    "   - Use of hashtags, emojis, and formatting\n",
    "   - Engagement techniques and calls-to-action\n",
    "2. Generate a new LinkedIn post about the given topic that matches:\n",
    "   - The identified writing style and tone\n",
    "   - Similar structure and formatting choices\n",
    "   - Equivalent use of platform features and hashtags\n",
    "   - Comparable engagement elements\n",
    "3. Return only the generated post, formatted exactly as it would appear on LinkedIn, without any additional commentary or explanations.\"\"\"\n",
    "\n",
    "system_prompts = {\"concise\": system_prompt_1, \"detailed\": system_prompt_2}\n",
    "\n",
    "\n",
    "clients = {\"openai\": OpenAI(), \"gemini\": OpenAI(\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "951cf5a3-2c8a-4926-b62a-df029ffdb5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, let's generate our dataset. We will use the `webpage_to_markdown` function to generate a set of ten different sources for posts. We will then generate four different posts for each source, using two different prompts and two different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8589656d-67c1-433b-b7db-6a932be7e93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow_pages = {\"Tutorial: Custom GenAI Models using ChatModel\": \"https://mlflow.org/docs/latest/llms/chat-model-guide/index.html\",\n",
    "                \"MLflow Tracing Schema\": \"https://mlflow.org/docs/latest/llms/tracing/tracing-schema.html\",\n",
    "                \"MLflow AI Gateway (Experimental)\": \"https://mlflow.org/docs/latest/llms/deployments/index.html\",\n",
    "                \"MLflow LLM Evaluation\": \"https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\",\n",
    "                \"LLM Evaluation with MLflow Example Notebook\": \"https://mlflow.org/docs/latest/llms/llm-evaluate/notebooks/question-answering-evaluation.html\",\n",
    "                \"MLflow Tracing for LLM Observability\": \"https://mlflow.org/docs/latest/llms/tracing/index.html\",\n",
    "                \"Deep Learning\": \"https://mlflow.org/docs/latest/deep-learning/index.html\",\n",
    "                \"DSPy Quickstart\": \"https://mlflow.org/docs/latest/llms/dspy/notebooks/dspy_quickstart.html\",\n",
    "                \"Building Custom Python Function Models with MLflow\": \"https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html\",\n",
    "                \"Quickstart with MLflow PyTorch Flavor\": \"https://mlflow.org/docs/latest/deep-learning/pytorch/quickstart/pytorch_quickstart.html\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b29c885e-99f7-45c8-b310-b8aae1cc59dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate the Evaluation Dataset\n",
    "\n",
    "\n",
    "Now that we have set up our experimental variables and identified the sources we want to use for post generation, we can generate an evaluation dataset. For each source we will generate four different posts, using two different system prompts and two different models. We will record:\n",
    "\n",
    "- the model used\n",
    "- the system prompt used\n",
    "- the source webpage content\n",
    "- the generated post\n",
    "- the example posts used (even though we aren't comparing different examples in the experiment, we still need to include them in the dataset for the evaluation suite to work because they are used by the style similarity metric).\n",
    "\n",
    "We will also use some slightly more detailed tracing as we generate this evaluation dataset—we will trace the markdown conversion, the prompt generation, and the post generation. This way, if we encounter any issues generating the evaluation dataset, we can easily identify the source of the issue.\n",
    "\n",
    "\n",
    "Here's the function we use to generate the evaluation dataset. Note that, to get the detailed tracing, we added `@mlflow.trace(span_type=\"FUNCTION\")` decorators to the `webpage_to_markdown` and `generate_prompt` functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978420a6-ac6f-42ce-8fae-16f6f6aeeb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "[\"tr-12fcc941e32d4b05b72b1b05b0020524\", \"tr-4f20f4162f1f44419b8d37a276128ba0\", \"tr-378d52c602cb4b83a451d1d2f90e0e4a\", \"tr-04573910a8f046dd84bdfa72434790ee\", \"tr-5a2934b3320943a1b90ba7bdf8d7d723\", \"tr-67ef6bbe01434bf4989e55ebe038d6ef\", \"tr-ba03a9d4912b47c49a58d93bd0b65cb3\", \"tr-ef544575b3664c7a892ba7728af4bc4f\", \"tr-98b58fdf965c446daa983aa48dc6463e\", \"tr-24f1c9306a214287886b86e292c75b09\"]",
      "text/plain": [
       "[Trace(request_id=tr-12fcc941e32d4b05b72b1b05b0020524), Trace(request_id=tr-4f20f4162f1f44419b8d37a276128ba0), Trace(request_id=tr-378d52c602cb4b83a451d1d2f90e0e4a), Trace(request_id=tr-04573910a8f046dd84bdfa72434790ee), Trace(request_id=tr-5a2934b3320943a1b90ba7bdf8d7d723), Trace(request_id=tr-67ef6bbe01434bf4989e55ebe038d6ef), Trace(request_id=tr-ba03a9d4912b47c49a58d93bd0b65cb3), Trace(request_id=tr-ef544575b3664c7a892ba7728af4bc4f), Trace(request_id=tr-98b58fdf965c446daa983aa48dc6463e), Trace(request_id=tr-24f1c9306a214287886b86e292c75b09)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "def generate_evaluation_dataset(system_prompts: dict, clients: dict, user_instruction: str, example_posts: list, context_pages: dict):\n",
    "    # Create list to store results\n",
    "    results = []\n",
    "\n",
    "    for page_name, page_url in context_pages.items():\n",
    "        for prompt_name, system_prompt in system_prompts.items():\n",
    "            for client_name, client in clients.items():\n",
    "\n",
    "                with mlflow.start_span(name=\"eval_dataset_generation\",\n",
    "                                       span_type=\"CHAIN\",\n",
    "                                       ) as parent_span:\n",
    "                    model = \"gpt-4o\" if client_name == \"openai\" else \"gemini-2.0-flash-exp\"\n",
    "                    parent_span.set_inputs({\"model\": model, \"system_prompt\": system_prompt,\n",
    "                                            \"example_post\": page_name})\n",
    "                    page_content = webpage_to_markdown(page_url)\n",
    "                    messages = generate_prompt(system_prompt, user_template, example_posts, page_content, additional_instructions)\n",
    "\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                    )\n",
    "\n",
    "\n",
    "                    results.append({\n",
    "                        'model': client_name,\n",
    "                        'system_prompt': prompt_name,\n",
    "                        'context_page': page_name,\n",
    "                        'user_instruction': user_instruction,\n",
    "                        'output': response.choices[0].message.content\n",
    "                    })\n",
    "                    # wait for 1 second\n",
    "                    #time.sleep(1)\n",
    "                    parent_span.set_outputs({\"output\": response.choices[0].message.content})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "user_instruction = \"Create a LinkedIn post about the benefits of using MLflow for machine learning projects.\"\n",
    "\n",
    "\n",
    "eval_dataset = generate_evaluation_dataset(system_prompts, clients, user_instruction, example_posts, mlflow_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442d5eae-e489-400d-b2fb-62c5ab2f38ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>context_page</th>\n",
       "      <th>user_instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai</td>\n",
       "      <td>concise</td>\n",
       "      <td>Tutorial: Custom GenAI Models using ChatModel</td>\n",
       "      <td>Create a LinkedIn post about the benefits of u...</td>\n",
       "      <td>New Tutorial Alert: Dive into Custom GenAI Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini</td>\n",
       "      <td>concise</td>\n",
       "      <td>Tutorial: Custom GenAI Models using ChatModel</td>\n",
       "      <td>Create a LinkedIn post about the benefits of u...</td>\n",
       "      <td>New tutorial: Build custom GenAI models with M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai</td>\n",
       "      <td>detailed</td>\n",
       "      <td>Tutorial: Custom GenAI Models using ChatModel</td>\n",
       "      <td>Create a LinkedIn post about the benefits of u...</td>\n",
       "      <td>🎓 New Tutorial Alert: Creating Custom GenAI Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini</td>\n",
       "      <td>detailed</td>\n",
       "      <td>Tutorial: Custom GenAI Models using ChatModel</td>\n",
       "      <td>Create a LinkedIn post about the benefits of u...</td>\n",
       "      <td>New tutorial alert! 🚨 Learn how to build custo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  ...                                             output\n",
       "0  openai  ...  New Tutorial Alert: Dive into Custom GenAI Mod...\n",
       "1  gemini  ...  New tutorial: Build custom GenAI models with M...\n",
       "2  openai  ...  🎓 New Tutorial Alert: Creating Custom GenAI Mo...\n",
       "3  gemini  ...  New tutorial alert! 🚨 Learn how to build custo...\n",
       "\n",
       "[4 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6660062f-c172-4421-8547-e398b0a8c750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset['examples'] = [example_posts] * len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9252713-8a7f-4dc0-aa3d-a24342829b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run the Evaluation Suite on the Evaluation Dataset\n",
    "\n",
    "Now that we have our evaluation dataset, we can run the evaluation suite on it. We will use the `mlflow.evaluate()` function to evaluate the generated posts using our three metrics.\n",
    "\n",
    "We will group the evaluation by model and system prompt, and then evaluate each group. Organizing the evaluation into separate runs gives us the easiest way to view the evaluation results in the MLflow UI.\n",
    "\n",
    "There are a few important things to note here:\n",
    "\n",
    "- We listed our metrics in the `extra_metrics` argument. This is because we are not using the default metrics associated with a particular task type and are instead using only the specific metrics we decided on for our use case.\n",
    "- In the `evaluator_config` argument, we set the `col_mapping` to map the inputs and context to the columns in our evaluation dataset. This allows the evaluation suite to correctly map the inputs and context to the evaluation results.\n",
    "- For the sake of organization, we created a parent run and nested each of the evaluation runs within it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf0dbbb-5b63-44c7-b2c0-b36faac1a307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"MLflow's GenAI evaluation metrics now work as callable functions as of MLflow 2.17, making them easier to use and integrate.\\nNow you can use metrics like answer_relevance, answer_correctness, faithfulness, and toxicity directly as functions—no need to go through mlflow.evaluate() anymore if you're just prototyping with individual metrics or integrating metric calls into systems where mlflow.evalaute is not necessary.\\nThis means:\\n🔍 Easier debugging during prototyping\\n🔌 More flexible integration options\\n🎯 Works with or without other MLflow features\\nCheck it out in action ⬇️\\nLearn more:\\n📚 Docs: https://lnkd.in/gyBzcrDr\\n📝 Release notes: https://lnkd.in/gBrNQfFC\\n#MachineLearning #AI #LLMs #LLMOps #Evals\",\n",
       " \"If you're already building with Python ML libraries, adding mlflow.autolog() to your code instantly gives you production-grade experiment tracking, model management, and observability—no extra infrastructure or code changes needed.\\nThe automatic logging works across a remarkable breadth of libraries—from GenAI frameworks like LangChain, OpenAI, and LlamaIndex to traditional ML and deep learning libraries like PyTorch, scikit-learn, and Fastai.\\nMLflow's autolog feature changes this equation. With a single line—mlflow.autolog()—you get automatic logging of:\\n📊 Training metrics and parameters for scikit-learn, PyTorch, many other ML frameworks\\n🔄 LLM traces, prompts, responses, and tool calls for OpenAI and LangChain\\n📦 Model signatures and artifacts\\n💾 Dataset information and example inputs\\nThe best part is that it works out of the box with the most popular libraries in the Python ML ecosystem: no need to modify your existing training code or add manual logging statements.\\nRead more: https://lnkd.in/e_aTp6HH\\n#machinelearning #mlops #ai #llmops\",\n",
       " \"New tutorial: Step-by-step guide to building a tool-calling LLM application using MLflow's ChatModel wrapper and tracing system.\\nThis tutorial shows you how to:\\n🔧 Create a tool-calling model using mlflow.pyfunc.ChatModel\\n🔄 Implement OpenAI function calling with automatic input/output handling\\n🔍 Add comprehensive tracing to debug multi-step LLM interactions\\n🚀 Deploy your model with full MLflow lifecycle management\\nThe guide includes a practical example building a weather information agent, showing how ChatModel simplifies complex LLM patterns while providing enterprise-grade observability.\\nCheck out the complete tutorial here: https://lnkd.in/gdTw8N2S\\n#MLOps #AIEngineering #LLMOps #AI\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "658f83f3-235b-49dd-849b-cbc4809b6e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a35fc6-5a41-49ef-bd1d-f2e2c5e722b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:22:31 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cbe197883d4a96868f72ee443d2cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa4459222a2484e94f7ff0ebd752176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:22:37 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:22:37 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a97b78d8bed4471a73ce5acfb2c2162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efff834cace64dbd88d73d1517dfb3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:22:46 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:22:46 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n2025/04/13 17:22:56 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355203d9c89c46a187d8fc226c36945c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf1d21fded432a8b395e4e55b13743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:02 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:02 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4ea7a3f3384b25828aea6a341f97d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637153afc6c14d2bbc8f333ff3f36a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:10 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:10 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n2025/04/13 17:23:20 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c693915e9c452bb18ee091a5ede728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f03c49b9554e919d9a74eda0338865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:27 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:27 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f7f128e8b843a4ae0afdb0af80a5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6f5bbd002744ef82262731027ed667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:34 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:34 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n2025/04/13 17:23:45 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c361501b9e34c1db8d3f77d47821172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d339312d2e4b80ac1dfbe4c24967ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:50 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:50 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffed2bbca0384e9fa6e02f95ecff1d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e023b3dd8644a786ae08defe44fe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/13 17:23:57 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n2025/04/13 17:23:57 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 2 in the `extra_metrics` parameter because it returned None.\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "mlflow.set_tracking_uri(\"file:./../mlruns\")\n",
    "mlflow.set_experiment(\"/Shared/genai-social\")\n",
    "\n",
    "# Get unique combinations of model and system_prompt\n",
    "model_prompts = eval_dataset[['model', 'system_prompt']].drop_duplicates()\n",
    "\n",
    "# Create parent run for all evaluations\n",
    "with mlflow.start_run(run_name=f\"social-post-generation-eval-{uuid.uuid4()}\") as parent_run:\n",
    "    mlflow.log_param(\"evaluation_type\", \"social_post_generation\")\n",
    "\n",
    "    # Evaluate each configuration subset\n",
    "    for _, row in model_prompts.iterrows():\n",
    "        model = row['model']\n",
    "        system_prompt = row['system_prompt']\n",
    "\n",
    "        # Subset the data for this configuration\n",
    "        subset_df = eval_dataset[\n",
    "            (eval_dataset['model'] == model) & \n",
    "            (eval_dataset['system_prompt'] == system_prompt)\n",
    "        ]\n",
    "\n",
    "        # Create child run for this specific configuration\n",
    "        with mlflow.start_run(run_name=f\"{model}-{system_prompt}\", nested=True) as child_run:\n",
    "            # Log configuration parameters\n",
    "            mlflow.log_params({\n",
    "                \"model\": model,\n",
    "                \"system_prompt\": system_prompt,\n",
    "            })\n",
    "\n",
    "            # Run evaluation\n",
    "            eval_results = mlflow.evaluate(\n",
    "                data=subset_df,\n",
    "                predictions=\"output\",\n",
    "                extra_metrics=[\n",
    "                    style_similarity_metric,\n",
    "                    faithfulness_metric,\n",
    "                    toxicity_metric\n",
    "                ],\n",
    "                evaluator_config={\n",
    "                    \"col_mapping\": {\n",
    "                        \"inputs\": \"user_instruction\",\n",
    "                        \"context\": \"context_page\",\n",
    "                        \"examples\": \"examples\"   \n",
    "                    }\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb74777-16c1-4c72-b5a3-e44b875ce61e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The evaluation UI gives us a number of ways to view the results. We can, for example, select the relevant runs, group by the context page name, and compare the faithfulness metric in order to see how well each model/system prompt combination performed on each page.\n",
    "\n",
    "We can also load the evaluation results into a Pandas DataFrame for further custom analysis. Let's get the last five run IDs (parent run + four child runs), load the evaluation results into a DataFrame, and then analyze the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da022c65-8ff1-4dad-9bb7-1473be65766c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Style Similarity  Faithfulness  Toxicity\nmodel  system_prompt                                          \ngemini concise                     4.0           1.1       NaN\n       detailed                    4.0           1.0       NaN\nopenai concise                     4.0           1.0       NaN\n       detailed                    4.0           1.0       NaN\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the MLflow client\n",
    "mlflow_client = mlflow.MlflowClient()\n",
    "\n",
    "# Get recent runs\n",
    "recent_runs = mlflow_client.search_runs(\n",
    "    experiment_ids=[mlflow.get_experiment_by_name(\"/Shared/genai-social\").experiment_id],\n",
    "    max_results=5,\n",
    "    order_by=[\"start_time DESC\"]\n",
    ")\n",
    "\n",
    "# Extract relevant data from the runs\n",
    "data = []\n",
    "for run in recent_runs:\n",
    "    run_data = {\n",
    "        'run_id': run.info.run_id,\n",
    "        'model': run.data.params.get('model'),\n",
    "        'system_prompt': run.data.params.get('system_prompt'),\n",
    "        'style_similarity/v1/score': run.data.metrics.get('style_similarity/v1/mean'),\n",
    "        'faithfulness/v1/score': run.data.metrics.get('faithfulness/v1/mean'),\n",
    "        'toxicity/v1/score': run.data.metrics.get('toxicity/v1/mean')\n",
    "    }\n",
    "    data.append(run_data)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'model' and 'system_prompt' and calculate the mean of the scores\n",
    "summary_df = df.groupby(['model', 'system_prompt']).agg({\n",
    "    'style_similarity/v1/score': 'mean',\n",
    "    'faithfulness/v1/score': 'mean',\n",
    "    'toxicity/v1/score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = ['Style Similarity', 'Faithfulness', 'Toxicity']\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be8c761-abcb-43ed-ae61-5e00ac3e3bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The ability to load the evaluation results into a Pandas DataFrame also allows us to use our preferred plotting libraries to visualize the results. For example, the following was created using `matplotlib`.\n",
    "\n",
    "The model and prompt combinations perform relatively similarly. Based on this experiment, the `gemini-2.0-flash-exp` model with the `detailed` system prompt appears to offer the best combination of style similarity and faithfulness. In a production setting, we would want to run a larger experiment to confirm this result.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this second part of a four-part guide detailing how MLflow integrates with a GenAI project, from conception through deployment, we evaluated our AI system. In particular, we saw how:\n",
    "\n",
    "- MLflow's callable metrics let us quickly and easily test the metrics we wanted to use to evaluate our system\n",
    "- `mlflow.evaluate()` can be used to run an evaluation suite, including two LLM-as-judge metrics, on our AI system.\n",
    "\n",
    "In the next section, we will see how to encapsulate our our application logic in a custom MLflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6049d70-0494-4227-a4f9-c3afec7cc287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cc6192a-d00f-4cea-83c3-971b1441eb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Part 3: Custom MLflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58866dbf-6101-471b-8326-02d95e5521aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow GenAI 1-2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}